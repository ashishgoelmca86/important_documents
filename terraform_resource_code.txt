 
# DynamoDB Table for File Processing Audit
resource "aws_dynamodb_table" "file_processing_audit" {
  name         = var.file_processing_audit_table_name
  billing_mode   = var.billing_mode
  read_capacity  = var.read_capacity
  write_capacity = var.write_capacity
  hash_key     = var.file_processing_audit_hash_key
  range_key    = var.file_processing_audit_range_key
 
  attribute {
    name = var.file_processing_audit_hash_key
    type = "S"  # Assuming the type is String
  }
 
  attribute {
    name = var.file_processing_audit_range_key
    type = "S"  # Assuming the type is String
  }
 
  tags = var.tags
}
 
# DynamoDB Table for Glue Job Audit
resource "aws_dynamodb_table" "glue_job_audit" {
  name         = var.glue_job_audit_table_name
  billing_mode   = var.billing_mode
  read_capacity  = var.read_capacity
  write_capacity = var.write_capacity
  hash_key     = var.glue_job_audit_hash_key
  range_key    = var.glue_job_audit_range_key
 
  attribute {
    name = var.glue_job_audit_hash_key
    type = "S"  # Assuming the type is String
  }
 
  attribute {
    name = var.glue_job_audit_range_key
    type = "S"  # Assuming the type is String
  }
 
  tags = var.tags
}
 
 
 
 
variable "billing_mode" {
  description = "value"
  type = string
  default = "PROVISIONED"
}
 
variable "read_capacity" {
   description = "value"
   type = number
   default = 20
}
 
variable "write_capacity" {
   description = "value"
   type = number
   default = 20
}
variable "file_processing_audit_table_name" {
  description = "Name of the DynamoDB table for file processing audit."
  type        = string
  # default     = "pse_dl_file_processing_audit"
}
 
variable "glue_job_audit_table_name" {
  description = "Name of the DynamoDB table for Glue job audit."
  type        = string
  # default     = "pse_dl_glue_job_audits"
}
 
variable "file_processing_audit_hash_key" {
  description = "Hash key for the FileProcessingAudit DynamoDB table."
  type        = string
  default     = "file_id"
}
 
variable "file_processing_audit_range_key" {
  description = "Range key for the FileProcessingAudit DynamoDB table."
  type        = string
  default     = "timestamp"
}
 
variable "glue_job_audit_hash_key" {
  description = "Hash key for the GlueJobAudit DynamoDB table."
  type        = string
  default     = "job_name"
}
 
variable "glue_job_audit_range_key" {
  description = "Range key for the GlueJobAudit DynamoDB table."
  type        = string
  default     = "job_run_id"
}
 
variable "tags" {
  description = "Tags to apply to resources."
  type        = map(string)
  default = {
    Environment = "dev"
    Project     = "DataEngineering"
  }
}
 
 
resource "aws_cloudwatch_event_rule" "s3_object_created" {
  name        = var.event_rule_name
  description = "Capture all object creation events in S3 bucket"
  event_pattern = <<PATTERN
{
  "source": [
    "aws.s3"
  ],
  "detail-type": [
    "AWS API Call via CloudTrail"
  ],
  "detail": {
    "eventName": [
      "PutObject",
      "CompleteMultipartUpload"
    ],
    "requestParameters": {
      "bucketName": [
        "${var.bucket_name[0]}"
      ]
    }
  }
}
PATTERN
}
 
resource "aws_iam_role" "s3_event_to_sqs_role" {
  name = "s3-event-to-sqs-role"
 
  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Action = "sts:AssumeRole"
      Effect = "Allow"
      Principal = {
        Service = "events.amazonaws.com"
      }
    }]
  })
}
 
resource "aws_iam_role_policy" "s3_event_to_iam_policy" {
  name   = "s3-event-to-sqs-policy"
  role   = aws_iam_role.s3_event_to_sqs_role.id
  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Action = [
          "sqs:SendMessage"
        ]
        Effect   = "Allow"
        Resource = "${var.sqs_queue_arn}"
      }
    ]
  })
}
 
resource "aws_sqs_queue_policy" "sqs_queue_policy" {
  depends_on = [ aws_cloudwatch_event_rule.s3_object_created ]
  queue_url = var.sqs_queue_url
  policy = jsonencode({
    Version = "2012-10-17",
    Statement = [
      {
        Effect = "Allow",
        Principal = {
          Service = "events.amazonaws.com"
        },
        Action = "sqs:SendMessage",
        Resource = "${var.sqs_queue_arn}",
        Condition = {
          ArnEquals = {
            "aws:SourceArn": aws_cloudwatch_event_rule.s3_object_created.arn
          }
        }
      }
    ]
  })
}
 
resource "aws_cloudwatch_event_target" "sqs_target" {
  depends_on = [ aws_cloudwatch_event_rule.s3_object_created, aws_sqs_queue_policy.sqs_queue_policy]
  rule      = aws_cloudwatch_event_rule.s3_object_created.name
  arn       = var.sqs_queue_arn
}
 
 
# resource "aws_lambda_permission" "allow_cloudwatch" {
#   statement_id  = "AllowExecutionFromCloudWatch"
#   action        = "lambda:InvokeFunction"
#   function_name = var.aws_lambda_function_name
#   principal     = "events.amazonaws.com"
#   source_arn    = aws_cloudwatch_event_rule.s3_object_created.arn
# }
 
 


 
variable "region" {
  description = "The AWS region to create resources in."
  type        = string
  default     = "us-west-2"
}
 
variable "bucket_name" {
  description = "The name of the S3 bucket."
  type        = list(string)
}
 
// variable "sqs_queue_name" {
//   description = "The name of the SQS queue."
//   type        = string
// }
 
variable "event_rule_name" {
  description = "The name of the EventBridge rule."
  type        = string
}
 
# variable "event_target_id" {
#   description = "The ID of the EventBridge target."
#   type        = string
# }
 
variable "sqs_queue_arn" {
  type = string
 
}
 
variable "sqs_queue_url" {
  type = string
 
}
# variable "aws_lambda_function_name" {
#   type = string
# }
 
 
data "aws_caller_identity" "current" {}
data "aws_region" "current" {}
 
# Step 1: Create a Glue Database
resource "aws_glue_catalog_database" "glue_database" {
  count = length(var.glue_database_name)
  name  = var.glue_database_name[count.index]
}
 
# Step 2: Create a Glue Crawler
resource "aws_glue_crawler" "glue_crawler" {
  count = length(var.glue_crawler_name)
  name  = var.glue_crawler_name[count.index]
  database_name = aws_glue_catalog_database.glue_database[count.index].name
  role  = "arn:aws:iam::${data.aws_caller_identity.current.id}:role/CI-DataEngineeringRole" # Replace with your IAM role ARN
 
  s3_target {
    path = var.bucket_name[0] # If var.bucket_name is a list
    
  }
}
 
# Step 3: Create a Glue Job
resource "aws_glue_job" "glue_job" {
  count = length(var.glue_job_name)
  name  = var.glue_job_name[count.index]
  role_arn = "arn:aws:iam::${data.aws_caller_identity.current.id}:role/CI-DataEngineeringRole" # Replace with your IAM role ARN
 
  command {
    name           = "glueetl"
    script_location = "${var.glue_script_s3}/${var.glue_job_script_path[count.index]}" # Replace with your script location
    python_version = "3"
  }
    default_arguments = local.glue_job_default_arguments[count.index]
 
  tags = {
    Name = var.glue_job_name[count.index]
  }
 
  # Choose either `worker_type` and `number_of_workers` or `max_capacity`
 
  # Option 1: Using `worker_type` and `number_of_workers`
  worker_type       = "G.1X"
  number_of_workers = 4
 
  # Option 2: Using `max_capacity`
  # max_capacity      = 2.0 # Adjust based on your needs
}
 
# IAM Role for Glue Jobs
resource "aws_iam_role" "glue_job_roles" {
  count = length(var.glue_job_name)
  name = "${var.glue_job_name[count.index]}-role"
  assume_role_policy = jsonencode({
    Version = "2012-10-17",
    Statement = [
      {
        Effect    = "Allow",
        Principal = {
          Service = "glue.amazonaws.com"
        },
        Action = "sts:AssumeRole"
      }
    ]
  })
}
 
# IAM Policies for Glue Jobs
resource "aws_iam_policy" "glue_job_policies" {
  count = length(var.glue_job_name)
  name        = "${var.glue_job_name[count.index]}-policy"
  description = "IAM policy for ${var.glue_job_name[count.index]}"
 
  policy = jsonencode({
    Version = "2012-10-17",
    Statement = [
      {
        Effect = "Allow",
        Action = [
          "s3:GetObject",
          "s3:PutObject",
          "s3:ListBucket"
        ],
        Resource = [
          "arn:aws:s3:::${var.bucket_name[0]}",
          "arn:aws:s3:::${var.bucket_name[0]}/*"
        ]
      },
      {
        Effect = "Allow",
        Action = [
          "logs:CreateLogGroup",
          "logs:CreateLogStream",
          "logs:PutLogEvents"
        ],
        Resource = "arn:aws:logs:*:*:*"
      },
      {
        Effect = "Allow",
        Action = [
          "glue:*"
        ],
        Resource = "*"
      }
    ]
  })
}
 
# Attach IAM Policy to the Role
resource "aws_iam_role_policy_attachment" "glue_job_policy_attachments" {
  count = length(var.glue_job_name)
  role       = aws_iam_role.glue_job_roles[count.index].name
  policy_arn = aws_iam_policy.glue_job_policies[count.index].arn
}
 
locals{
  # count = length(var.glue_job_script_path)
glue_job_default_arguments = [
  {
     "--TempDir"             = "s3://pse-sapecc-glue-assets-tf/scripts/temporary/"  #Need to update path to take scripts from terraform directory
    "--job-bookmark-option" = "job-bookmark-enable"
    "--extra-py-files"      = "s3://pse-sapecc-glue-assets-tf/utility/common_functions.py"
    "--BUCKET_NAME"           = "pse-datalake-source-420393866958-us-west-2"
    "--DQ_BUCKET_NAME"        = "pse-sapecc-dl-dqlogs-dev"
    "--DYNAMODB_TABLE_NAME"   = "pse_dl_glue_job_audits"
    "--PROCESSED_FILE_PATH" = "s3://pse-sapecc-dl-processed-dev/processed-sap-circuit/"
    "--RAW_DATABASE_NAME"  = "pse-sapecc-raw-db"
 
  },
  {
    "--ANALYTICS_BUCKET_NAME"         = "pse-sapecc-dl-analytics-dev-tf"
    "--ANALYTICS_DATABASE"      = "pse-sapecc-analytics-db"
    "--ANALYTICS_FULLPATH" = "s3://pse-sapecc-dl-analytics-dev/analytics-sap-circuit/distribution_circuits_sap/"
    "--DYNAMODB_TABLE_NAME" = "pse_dl_glue_job_audits"
    "--PROCESSED_DATABASE" = "pse-sapecc-processed-db"
  },
  {
     "--TempDir"             = "s3://pse-sapecc-glue-assets-tf/scripts/temporary/"  #Need to update path to take scripts from terraform directory
    "--job-bookmark-option" = "job-bookmark-enable"
    "--extra-py-files"      = "s3://pse-sapecc-glue-assets-tf/utility/common_functions.py"
    "--BUCKET_NAME"           = "pse-datalake-source-420393866958-us-west-2"
    "--DQ_BUCKET_NAME"        = "pse-sapecc-dl-dqlogs-dev-tf"
    "--DYNAMODB_TABLE_NAME"   = "pse_dl_glue_job_audits"
    "--PROCESSED_FILE_PATH" = "s3://pse-sapecc-dl-processed-dev/processed_sap_consolidated/"
    "--RAW_DATABASE_NAME"  = "pse-sapecc-raw-db"
 
  },
  {
    "--ANALYTICS_DATABASE"      = "pse-sapecc-analytics-db"
    "--ANALYTICS_FOLDER_PATH" = "s3://pse-sapecc-dl-analytics-dev/analytics-sap-consolidated/"
    "--DYNAMODB_TABLE_NAME" = "pse_dl_glue_job_audits"
    "--PROCESSED_DATABASE" = "pse-sapecc-processed-db"
  }
]
 
}
 
variable "bucket_name" {
  description = "the name of your bucket name, e.g. \"datalake\""
  type        = list(string)
}
 
variable "glue_crawler_name" {
  description = "the name of your glue crawler name, e.g. \"datalake\""
  type        = list(string)
}
 
 variable "glue_job_name" {
   description = "the name of your glue job name, e.g. \"datalake\""
   type        = list(string)
 }
 
 variable "glue_job_script_path" {
  description = "The S3 path to the Python script that AWS Glue executes."
  type        = list(string)
}
 
 variable "glue_script_s3" {
  description = "The S3 bucket to the Python script that AWS Glue executes."
  type        = string
}
 
variable "glue_python_function_path" {
   description = "The S3 bucket to the Python script that AWS Glue executes."
  type        = string
}
 
#  variable "glue_job_default_arguments" {
#     description = "Default arguments for the Glue jobs, including specific settings for each job."
#     type        =   list(string)
# }
 
 
 
  
variable "glue_database_name" {
   description = "The S3 path to the Python script that AWS Glue executes."
  type        = list(string)
}
 
# variable "glue_job_audit_table_name" {
#   description = "Name of the DynamoDB table for Glue job audit."
#   type        = string
#   # default     = "pse_dl_glue_job_audits"
# }
# variable "glue_role_arn" {
#   description = "IAM role ARN for Glue"
#   type        = string
# }
 
# variable "s3_bucket_data" {
#   description = "S3 bucket path for data"
#   type        = string
# }
 
# variable "s3_bucket_scripts" {
#   description = "S3 bucket path for scripts"
#   type        = string
# }
 
# variable "glue_script" {
#   description = "Path to the Glue ETL script"
#   type        = string
# }
 
# variable "temp_dir" {
#   description = "Temporary directory for Glue job"
#   type        = string
# }
 
 
 
 
 
output "glue_database_name" {
  value = aws_glue_catalog_database.glue_database[*].name
}
 
output "glue_crawler_name" {
  value = aws_glue_crawler.glue_crawler[*].name
}
 
output "glue_job_name" {
  value = aws_glue_job.glue_job[*].name
}
 
output "glue_crawler_arn" {
  value = aws_glue_crawler.glue_crawler[*].arn
}
 
 
 
 
 
data "aws_iam_policy_document" "lambda_assume_role" {
  statement {
    effect = "Allow"
    actions = ["sts:AssumeRole"]
 
    principals {
      type        = "Service"
      identifiers = ["lambda.amazonaws.com"]
    }
  }
}
 
resource "aws_iam_role" "lambda_execution_role" {
  count = length(var.lambda_function_role_name)
  name  = var.lambda_function_role_name[count.index]
  assume_role_policy = data.aws_iam_policy_document.lambda_assume_role.json
}
 
resource "aws_iam_policy" "lambda_s3_policy" {
  count       = length(var.lambda_function_policy_name)
  name        = var.lambda_function_policy_name[count.index]
  description = "IAM policy for the Lambda function"
  policy      = jsonencode({
    Version = "2012-10-17",
    Statement = [
      {
        Effect   = "Allow",
        Action   = [
          "logs:CreateLogGroup",
          "logs:CreateLogStream",
          "logs:PutLogEvents",
        ],
        Resource = "arn:aws:logs:*:*:*"
      },
        {
        Effect   = "Allow",
        Action   = [
          "sqs:SendMessage",
          "sqs:ReceiveMessage",
          "sqs:DeleteMessage",
          "sqs:GetQueueAttributes"
        ],
        Resource = "*"
      },
      {
        Effect   = "Allow",
        Action   = [
          "s3:GetObject"
        ],
        Resource = "arn:aws:s3:::${var.bucket_name[0]}"
      },
      {
        Effect   = "Allow",
        Action   = [
          "sns:Publish"
        ],
        Resource = "*"
      }
    ]
  })
}
 
resource "aws_iam_role_policy_attachment" "lambda_s3_policy_attachment" {
  count      = length(var.lambda_function_role_name) # Assuming same count as IAM role
  role       = aws_iam_role.lambda_execution_role[count.index].name
  policy_arn = aws_iam_policy.lambda_s3_policy[count.index].arn
}
 
resource "aws_iam_role_policy_attachment" "lambda_policy_attachment" {
  count      = length(var.lambda_function_role_name) # Assuming same count as IAM role
  role       = aws_iam_role.lambda_execution_role[count.index].name
  policy_arn = "arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole"
}
 
resource "aws_iam_role_policy_attachment" "lambda_attachment_CI_policy" {
  count      = length(var.lambda_function_role_name)
  role       = aws_iam_role.lambda_execution_role[count.index].name
  policy_arn = var.CI-DataEngineeringPolicy_arn
}
 
data "archive_file" "lambda_code" {
  type        = "zip"
  source_file = "${path.module}/pse-sapcee-validate-dev.py"
  output_path = "${path.module}/pse-sapcee-validate-dev.zip"
}
 
resource "aws_lambda_function" "pse_lambda" {
  count = length(var.lambda_function_name)
  function_name = var.lambda_function_name[count.index]
  role          = aws_iam_role.lambda_execution_role[count.index].arn
  handler       = var.lambda_handler
  runtime       = var.lambda_runtime
  # s3_bucket     = var.bucket_name
  filename      = data.archive_file.lambda_code.output_path
 
  environment { 
    variables = {
    #variables = var.lambda_environment_variables
    BUCKET_NAME = var.bucket_name[0]
    DYNAMODB_TABLE = var.glue_job_audit_table_name
    ERROR_BUCKET_NAME = var.bucket_name[3]
    FILELOCPARAM = var.filelocparam
    SNS_TOPIC_ARN = var.sns_topic_arn
    STEP_FUNCTION_ARN = var.step_function_arn
   }
  }
  tags = {
    Name        = var.lambda_function_name[count.index]
    Environment = var.environment
  }
 
  depends_on = [aws_iam_role.lambda_execution_role]
}
 
resource "aws_lambda_event_source_mapping" "event_source_mapping" {
  event_source_arn = var.sqs_queue_arn
  enabled          = true
  function_name    = aws_lambda_function.pse_lambda[0].arn
  batch_size       = 10
}
 
resource "aws_lambda_permission" "example_lambda_sqs_permission" {
  statement_id  = "AllowSQSTrigger"
  action        = "lambda:InvokeFunction"
  function_name = aws_lambda_function.pse_lambda[0].function_name
  principal     = "sqs.amazonaws.com"
  source_arn    = var.sqs_queue_arn
}
 
 
 
 
 
 
output "lambda_function_arn" {
  description = "The ARN of the Lambda function"
  value       = aws_lambda_function.pse_lambda[0].arn
}
 


 
variable "lambda_runtime" {
  description = "The runtime for the lambda function"
  type        = string
}
 
// variable "lambda_environment_variables" {
//   description = "The environment variables for the lambda function"
//   type        = map(string)
// }
variable "glue_job_audit_table_name" {
  description = "Name of the DynamoDB table for Glue job audit."
  type        = string
  # default     = "pse_dl_glue_job_audits"
}
 
variable "environment" {
  description = "The environment where the lambda function will be deployed"
  type        = string
}
 
variable "bucket_name" {
  description = "The name of the S3 bucket to trigger the lambda function"
  type        = list(string)
}
 
 
variable "filelocparam" {
  description = "The name of the S3 bucket to trigger the lambda function"
  type        = string 
  
}
 
variable "sns_topic_arn" {
  description = "The name of the S3 bucket to trigger the lambda function"
  type        = string 
  
}
variable "step_function_arn" {
  description = "The name of the S3 bucket to trigger the lambda function"
  type        = string 
  
}
 
# variable "s3_bucket_arn" {
#   description = "The ARN of the S3 bucket to trigger the lambda function"
#   type        = string
# }
 
# variable "s3_filter_prefix" {
#   description = "The prefix filter for the S3 bucket notifications"
#   type        = string
#   default     = ""
# }
 
# variable "s3_filter_suffix" {
#   description = "The suffix filter for the S3 bucket notifications"
#   type        = string
#   default     = ""
# }
 
# variable "lambda_role_arn" {
#   description = "The ARN of the IAM role that Lambda assumes when it executes the function"
#   type        = list(string)
# }
 
 variable "lambda_function_name" {
   description = "The name of the lambda function"
   type        = list(string)
}
 
variable "lambda_function_role_name" {
  description = "The name of the IAM role for the lambda function"
  type        = list(string)
}
 
variable "lambda_handler" {
  description = "The handler for the lambda function"
  type        = string
}
 
 variable "lambda_function_policy_name" {
   description = "The name of the IAM role for the lambda function"
  type        = list(string)
 
 }
 
 variable "log_retention_days" {
  description = "Number of days to retain logs in CloudWatch"
  type        = number
  default     = 14
}
 
variable "sqs_queue_arn" {
  description = "arn of the sqs"
  type        = string
}
 
 variable "CI-DataEngineeringPolicy_arn" {
  description = "arn of the role policy"
  type = string
}
 
================
resource "aws_s3_bucket" "terraform_state" {
  count = length(var.bucket_name)
  bucket = "${var.bucket_name[count.index]}"
  tags = {
    Name        = var.name
    Environment = var.environment
    ProjectName= var.project_name
    WorkOrder= var.work_order
  
  }
}
resource "aws_s3_bucket_notification" "enable_event" {
   depends_on = [aws_s3_bucket.terraform_state[0]]
   bucket = aws_s3_bucket.terraform_state[0].id
   eventbridge = true
  
 }
 
resource "aws_s3_bucket_versioning" "enabled" {
  count = length(var.bucket_name)
  bucket = aws_s3_bucket.terraform_state[count.index].id
  versioning_configuration {
    status = "Enabled"
  }
}
 
resource "aws_s3_object" "folder" {
 
  bucket = aws_s3_bucket.terraform_state[0].id
  for_each = var.default_s3_content
  acl    = "private"
  key = "sap/${each.value}/"
  }
 
  resource "aws_s3_object" "folder2" {
  bucket = aws_s3_bucket.terraform_state[0].id
  key    = "esri/"  # Creates a "subfolder2" in the S3 bucket
  acl    = "private"
}
 
resource "aws_s3_bucket_server_side_encryption_configuration" "default" {
  count = length(var.bucket_name)
  bucket = aws_s3_bucket.terraform_state[count.index].id  
  rule {
    apply_server_side_encryption_by_default {
      kms_master_key_id = "arn:aws:kms:us-west-2:420393866958:key/d81fc091-ef05-4a84-a08e-aaa0095262b4"
      sse_algorithm     = "aws:kms"
    }
  }
}
 
resource "aws_s3_bucket_public_access_block" "public_access" {
  count = length(var.bucket_name)
  bucket = aws_s3_bucket.terraform_state[count.index].id  
  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true
}
 
 
resource "aws_s3_bucket_lifecycle_configuration" "bucket-config" {
  count = length(var.bucket_name)
  bucket = "${var.bucket_name[count.index]}"
 
  rule {
    id = "log"
 
    expiration {
      days = 90
    }
 
    filter {
      and {
        prefix = "log/"
 
        tags = {
          rule      = "log"
          autoclean = "true"
        }
      }
    }
 
    status = "Enabled"
 
    transition {
      days          = 30
      storage_class = "STANDARD_IA"
    }
 
    transition {
      days          = 60
      storage_class = "GLACIER"
    }
  }
 
  rule {
    id = "tmp"
 
    filter {
      prefix = "tmp/"
    }
 
    expiration {
      days = 365 
    }
 
    status = "Enabled"
  }
}
 
 
resource "aws_s3_bucket_policy" "terraform_state" {
  count = length(var.bucket_name)
  bucket = "${var.bucket_name[count.index]}"
  #  bucket = aws_s3_bucket.terraform_state.id
    policy = jsonencode({
        Version = "2012-10-17"
        Id      = "BUCKET-POLICY"
        Statement = [
            {
                Sid       = "EnforceTls"
                Effect    = "Deny"
                Principal = "*"
                Action    = "s3:*"
                Resource = [
                    "${aws_s3_bucket.terraform_state[count.index].arn}/*",
                    "${aws_s3_bucket.terraform_state[count.index].arn}",
                ]
                Condition = {
                    Bool = {
                        "aws:SecureTransport" = "false"
                    }
                }
            },
            {
                Sid       = "EnforceProtoVer"
                Effect    = "Deny"
                Principal = "*"
                Action    = "s3:*"
                Resource = [
                    "${aws_s3_bucket.terraform_state[count.index].arn}/*",
                    "${aws_s3_bucket.terraform_state[count.index].arn}",
                ]
                Condition = {
                    NumericLessThan = {
                        "s3:TlsVersion": 1.3
                    }
                }
            }
        ]
    })
}
 
  resource "aws_s3_object" "s3_glue_script_location" {
  depends_on = [ aws_s3_bucket.terraform_state ]
  count = length(var.glue_script_location_source_key)
  bucket      = aws_s3_bucket.terraform_state[5].id
  key    = var.glue_script_location_source_key[count.index]
  source = "${path.module}/sap/${var.glue_script_location_source_path[count.index]}"
  etag = filemd5("${path.module}/sap/${var.glue_script_location_source_path[count.index]}")
}
variable   "name"  {   description =  "the ... by Goel, Ashish - Accenture
Goel, Ashish - Accenture (External)
12:32 PM
variable "name" {
  description = "the name of your stack, e.g. \"datalake\""
  type = string
}
 
variable "environment" {
  description = "the name of your environment, e.g. \"dev\""
  type = string
}
 
 
variable "bucket_name" {
  description = "the name of your bucket name, e.g. \"datalake\""
  type = list(string)
}
 
variable "default_s3_content" {
  description = "The default content of the s3 bucket upon creation of the bucket"
  type = set(string)
}
 
variable "project_name" {
  description = "the name of your bucket name, e.g. \"datalake\""
  type = string
  # default = "datalake"
}
 
variable "work_order" {
  description = "the name of your bucket name, e.g. \"datalake\""
  type = string
}
 
 variable "glue_script_location_source_key" {
   description = "the name of your bucket name, e.g. \"datalake\""
   type = list(string)
 }
 
 variable "glue_script_location_source_path" {
   description = "the name of your bucket name, e.g. \"datalake\""
   type = list(string)
 }
 
// variable "topic_arn" {
//   description = "the name of the topic arn"
//   type = string
// }
  output   "bucket_name"  {   description =... by Goel, Ashish - Accenture
Goel, Ashish - Accenture (External)
12:33 PM
 
output "bucket_name" {
  description = "The name of the raw bucket"
  value       = aws_s3_bucket.terraform_state[0].bucket
}
 
output "processed_bucket" {
  description = "The name of the processed bucket"
  value       = aws_s3_bucket.terraform_state[1].bucket
}
 
output "analytics_bucket" {
  description = "The name of the processed bucket"
  value       = aws_s3_bucket.terraform_state[2].bucket
}
output "error_bucket_lambda" {
  description = "The name of the processed bucket"
  value       = aws_s3_bucket.terraform_state[3].bucket
}
 
resource   "aws_sns_topic"   "this"  {   na... by Goel, Ashish - Accenture
Goel, Ashish - Accenture (External)
12:33 PM
resource "aws_sns_topic" "this" {
  name = var.sns_topic_name
 
  tags = {
    Name        = var.sns_topic_name
    #Environment = var.environment
  }
}
 
resource "aws_sns_topic_subscription" "this" {
  topic_arn = aws_sns_topic.this.arn
  protocol  = "email"
  endpoint  = var.subscription_email
}
 
variable   "subscription_email"  {   descri... by Goel, Ashish - Accenture
Goel, Ashish - Accenture (External)
12:33 PM
variable "subscription_email" {
  description = "Email address for the SNS subscription"
  type        = string
}
 
 
variable "sns_topic_name" {
  description = "Name for the SNS Topic"
  type        = string
}
output   "sns_topic_arn"  {   description =... by Goel, Ashish - Accenture
Goel, Ashish - Accenture (External)
12:34 PM
output "sns_topic_arn" {
  description = "The ARN of the SNS topic"
   value       = aws_sns_topic.this.arn
 }
 
data   "aws_caller_identity"   "current"  {... by Goel, Ashish - Accenture
Goel, Ashish - Accenture (External)
12:34 PM
data "aws_caller_identity" "current" {}
data "aws_region" "current" {}
 
resource "aws_sqs_queue" "queue" {
  name = var.sqs_name
  visibility_timeout_seconds = var.timeout
 
  redrive_policy = jsonencode({
    deadLetterTargetArn = aws_sqs_queue.dlq.arn
    maxReceiveCount     = var.max_recievecount
  })
}
 
resource "aws_sqs_queue" "dlq" {
  name =  "${var.sqs_name}-dlq"
}
 
data "aws_iam_policy_document" "queue_policy" {
  statement {
    principals {
      type        = "AWS"
      identifiers = ["arn:aws:iam::${data.aws_caller_identity.current.account_id}:root"]
    }
 
    actions = ["SQS:*"]
    resources = [
      aws_sqs_queue.queue.arn
    ]
  }
}
 
resource "aws_sqs_queue_policy" "queue_policy" {
  queue_url = aws_sqs_queue.queue.id
  policy    = data.aws_iam_policy_document.queue_policy.json
  depends_on = [aws_sqs_queue.queue]
}
 
 
 
 
 
 
 
 
// resource "aws_sqs_queue" "my_queue" {
//   name                        = var.queue_name
//   delay_seconds               = var.delay_seconds
//   max_message_size            = var.max_message_size
//   message_retention_seconds   = var.message_retention_seconds
//   receive_wait_time_seconds   = var.receive_wait_time_seconds
//   visibility_timeout_seconds  = var.visibility_timeout_seconds
//   tags                        = var.tags
// }
variable   "sqs_name"  {   type = string } ... by Goel, Ashish - Accenture
Goel, Ashish - Accenture (External)
12:34 PM
variable "sqs_name" {
  type = string
}
variable "max_recievecount" {
  type    = number
  default = 3
}
variable "timeout" {
  type = number
}
   output  "sqs_queue_arn"  {    value =  a...


 
 output "sqs_queue_arn" {
   value =  aws_sqs_queue.queue.arn
 }
=========================

step function

resource "aws_iam_role" "step_functions_role" {
  name = "StepFunctionsRole"
  assume_role_policy = jsonencode({
    Version = "2012-10-17",
    Statement = [
      {
        Action = "sts:AssumeRole",
        Effect = "Allow",
        Principal = {
          Service = "states.amazonaws.com"
        }
      }
    ]
  })
}
 
resource "aws_iam_policy" "step_functions_policy" {
  name        = "StepFunctionsPolicy"
  description = "Policy for Step Functions to access AWS resources"
  policy = jsonencode({
    Version = "2012-10-17",
    Statement = [
      {
        Effect = "Allow",
        Action = [
          "logs:CreateLogGroup",
          "logs:CreateLogStream",
          "logs:PutLogEvents"
        ],
        Resource = "*"
      },
      {
        Effect = "Allow",
        Action = [
          "lambda:InvokeFunction"
        ],
        Resource = "${var.lambda_function_arn}"
      },
      {
        Effect = "Allow",
        Action = [
          "s3:PutObject"
        ],
        Resource = "arn:aws:s3:::${var.bucket_name[1]}/*"
      },
      {
        Effect = "Allow",
        Action = [
          "s3:PutObject"
        ],
        Resource = "arn:aws:s3:::${var.bucket_name[2]}/*"
      },
      {
        Effect = "Allow",
        Action = [
          "sns:Publish"
        ],
        Resource = "${var.sns_topic_arn}",
      },
      {
        Effect = "Allow",
        Action = [
          "glue:StartJobRun"
        ],
        Resource = ["arn:aws:glue:${var.region}:${var.account_id}:job/${var.glue_job_name[0]}",
        "arn:aws:glue:${var.region}:${var.account_id}:job/${var.glue_job_name[1]}",
        "arn:aws:glue:${var.region}:${var.account_id}:job/${var.glue_job_name[2]}",
        "arn:aws:glue:${var.region}:${var.account_id}:job/${var.glue_job_name[3]}"]
      },
      {
        Effect = "Allow",
        Action = [
          "events:PutEvents"
        ],
        Resource = "*"
      }
    ]
  })
}
 
resource "aws_iam_role_policy_attachment" "step_functions_attachment" {
  role       = aws_iam_role.step_functions_role.name
  policy_arn = aws_iam_policy.step_functions_policy.arn
}
 
resource "aws_iam_role_policy_attachment" "step_functions_attachment_CI_policy" {
  role       = aws_iam_role.step_functions_role.name
  policy_arn = var.CI-DataEngineeringPolicy_arn
}
 
resource "aws_sfn_state_machine" "step_function" {
  count     = length(var.step_function_name)
  name      = "${var.step_function_name[count.index]}"
  role_arn  = aws_iam_role.step_functions_role.arn
  definition = <<EOF
{
  "Comment": "Runs an ETL pipeline to process SAP source data from Raw layer all the way to the Analytics layer",
  "StartAt": "StartCrawler",
  "States": {
    "Fail": {
      "Type": "Fail"
    },
    "StartCrawler": {
      "Type": "Task",
      "Parameters": {
        "Name": "${var.glue_crawler_name[0]}"
      },
      "Resource": "arn:aws:states:::aws-sdk:glue:startCrawler",
      "Next": "crawler_info",
      "Catch": [
        {
          "ErrorEquals": [
            "States.ALL"
          ],
          "Next": "Fail",
          "ResultPath": "$.taskresult"
        }
      ],
      "ResultPath": "$.taskresult"
    },
    "crawler_info": {
      "Type": "Task",
      "Next": "crawler_status",
      "Parameters": {
        "Name":  "${var.glue_crawler_name[0]}"
      },
      "Resource": "arn:aws:states:::aws-sdk:glue:getCrawler",
      "Retry": [
        {
          "ErrorEquals": [
            "States.ALL"
          ],
          "BackoffRate": 2,
          "IntervalSeconds": 10,
          "MaxAttempts": 3
        }
      ]
    },
    "crawler_status": {
      "Type": "Choice",
      "Choices": [
        {
          "Variable": "$.Crawler.State",
          "StringEquals": "FAILED",
          "Next": "Fail"
        },
        {
          "Variable": "$.Crawler.State",
          "StringEquals": "RUNNING",
          "Next": "crawler_finish_wait"
        },
        {
          "Variable": "$.Crawler.State",
          "StringEquals": "STOPPING",
          "Next": "crawler_finish_wait"
        },
        {
          "Variable": "$.Crawler.State",
          "StringEquals": "SUCCESS",
          "Next": "Glue Jobs"
        }
      ],
      "Default": "Glue Jobs"
    },
    "Glue Jobs": {
      "Type": "Parallel",
      "Branches": [
        {
          "StartAt": "processed_circuit_job",
          "States": {
            "processed_circuit_job": {
              "Type": "Task",
              "Resource": "arn:aws:states:::glue:startJobRun.sync",
              "Parameters": {
                "JobName":  "${var.glue_job_name[0]}"
              },
              "Catch": [
                {
                  "ErrorEquals": [
                    "States.ALL"
                  ],
                  "Comment": "Processed layer Circuit job failed",
                  "ResultPath": "$.Status",
                  "Next": "Circuit SNS Publish"
                }
              ],
              "Next": "analytics_sap_circuit_data_dev"
            },
            "analytics_sap_circuit_data_dev": {
              "Type": "Task",
              "Resource": "arn:aws:states:::glue:startJobRun.sync",
              "Parameters": {
                "JobName":  "${var.glue_job_name[1]}"
              },
              "Catch": [
                {
                  "ErrorEquals": [
                    "States.ALL"
                  ],
                  "Comment": "Circuit job in Analytics layer is failed",
                  "ResultPath": "$.Status",
                  "Next": "Circuit SNS Publish"
                }
              ],
              "End": true
            },
            "Circuit SNS Publish": {
              "Type": "Task",
              "Resource": "arn:aws:states:::sns:publish",
              "Parameters": {
                "Message.$": "$",
                "TopicArn": "${var.sns_topic_arn}"
              },
              "End": true
            }
          }
        },
        {
          "StartAt": "processed_consolidated_job",
          "States": {
            "processed_consolidated_job": {
              "Type": "Task",
              "Resource": "arn:aws:states:::glue:startJobRun.sync",
              "Parameters": {
                "JobName":  "${var.glue_job_name[2]}"
              },
              "Catch": [
                {
                  "ErrorEquals": [
                    "States.ALL"
                  ],
                  "Comment": "Consolidated job in Proccessed layer is failed",
                  "ResultPath": "$.Status",
                  "Next": "Consolidated SNS Publish"
                }
              ],
              "Next": "analytics_sap_consolidated_data_dev",
              "ResultPath": "$.Status"
            },
            "analytics_sap_consolidated_data_dev": {
              "Type": "Task",
              "Resource": "arn:aws:states:::glue:startJobRun.sync",
              "Parameters": {
                "JobName": "${var.glue_job_name[3]}"
              },
              "Catch": [
                {
                  "ErrorEquals": [
                    "States.ALL"
                  ],
                  "Comment": "Analytics Consolidated job failed",
                  "ResultPath": "$.Status",
                  "Next": "Consolidated SNS Publish"
                }
              ],
              "End": true
            },
            "Consolidated SNS Publish": {
              "Type": "Task",
              "Resource": "arn:aws:states:::sns:publish",
              "Parameters": {
                "TopicArn": "${var.sns_topic_arn}",
                "Message.$": "$"
              },
              "End": true
            }
          }
        }
      ],
      "End": true,
      "ResultPath": "$.Status",
      "Catch": [
        {
          "ErrorEquals": [
            "States.ALL"
          ],
          "Next": "Fail",
          "ResultPath": "$.Status"
        }
      ]
    },
    "crawler_finish_wait": {
      "Type": "Wait",
      "Seconds": 30,
      "Next": "crawler_info"
    }
  }
}
EOF
}
 
 
variable "region" {
  description = "the AWS region"
  type        = string
  default = "us-west-2"
}
 
variable "bucket_name" {
  description = "the name of your bucket name, e.g. \"datalake\""
  type = list(string)
}
 
 variable "account_id" {
   description = "the Id of the aws account"
  type        = string
  default = "4203-9386-6958"
 }
 
variable "glue_crawler_arn" {
  description = "Arn of the glue crawler"
  type        = string
}
 
variable "step_function_name" {
   description = "the name of your step function name, e.g. \"datalake\""
 type        = list(string)
}
 
variable "glue_crawler_name" {
  description = "the name of your glue crawler name, e.g. \"datalake\""
  type        = list(string)
}
 
 variable "glue_job_name" {
   description = "the name of your glue job name, e.g. \"datalake\""
   type        = list(string)
 }
 
 variable "sns_topic_arn" {
  description = "the arn of the topic"
   type        = string
 }
 
 variable "lambda_function_arn" {
   description = "the arn of the lambda function"
   type        = string
 }
 
 variable "sqs_queue_arn" {
    description = "the arn of the sqs"
   type        = string
 }
 
 variable "CI-DataEngineeringPolicy_arn" {
  description = "arn of the role policy"
  type = string
}
 ==============================
main.tf

provider "aws" {
  region                   = var.region
  shared_config_files      = ["${var.shared_config_files}"]
  shared_credentials_files = ["${var.shared_credentials_files}"]
  profile                  = var.profile
}
data "aws_caller_identity" "current" {}
 
module "s3" {
  source       = "./terraform-modules/s3"
  name         = var.name
  environment  = var.environment
  bucket_name  = var.bucket_name
  project_name = var.project_name
  work_order   = var.work_order
  glue_script_location_source_key = var.glue_script_location_source_key
  glue_script_location_source_path = var.glue_script_location_source_path
  default_s3_content = var.default_s3_content
}
 
module "sqs" {
  source           = "./terraform-modules/sqs"
  sqs_name         = var.sqs_name
  max_recievecount = var.max_recievecount
  timeout          = var.timeout
}
 
module "event_bridge" {
  source          = "./terraform-modules/eventBridge"
  event_rule_name = var.event_rule_name
  sqs_queue_arn   = module.sqs.sqs_queue_arn
  sqs_queue_url   = var.sqs_queue_url
  bucket_name     = var.bucket_name
  region          = var.region
}
 
module "sns_topic" {
  source             = "./terraform-modules/sns"
  sns_topic_name     = var.sns_topic_name
  subscription_email = var.subscription_email
}
 
module "lambda_function" {
  depends_on = [module.s3]
  source     = "./terraform-modules/lambda"
  # lambda_role_arn              = var.lambda_role_arn
  lambda_function_name         = var.lambda_function_name
  lambda_function_role_name    = var.lambda_function_role_name
  lambda_function_policy_name  = var.lambda_function_policy_name
  lambda_handler               = var.lambda_handler
  lambda_runtime               = var.lambda_runtime
 # lambda_environment_variables = var.lambda_environment_variables
  environment                  = var.environment
 glue_job_audit_table_name     = var.glue_job_audit_table_name
  filelocparam                = var.filelocparam
  sns_topic_arn               = var.sns_topic_arn
  step_function_arn           = var.step_function_arn
  bucket_name                  = [module.s3.bucket_name,module.s3.analytics_bucket, module.s3.processed_bucket,module.s3.error_bucket_lambda]
  sqs_queue_arn                = module.sqs.sqs_queue_arn
  CI-DataEngineeringPolicy_arn = var.CI-DataEngineeringPolicy_arn
 
}
 
 
module "glue" {
  depends_on         = [module.s3]
  source             = "./terraform-modules/glue"
  glue_database_name = var.glue_database_name
  glue_crawler_name  = var.glue_crawler_name
  glue_job_name      = var.glue_job_name
  # glue_role_arn     = "arn:aws:iam::420393866958:role/CI-DataEngineeringRole" # Replace with your IAM role ARN
  # s3_bucket_data    = "s3://dataengineering-datalake-dev/" # Replace with your S3 path
  # s3_bucket_scripts = "s3://dataengineering-datalake-dev/scripts/" # Replace with your S3 path
  # glue_script       = "s3://dataengineering-datalake-dev/lambda_function.zip" # Replace with your script location
  # temp_dir          = "s3://dataengineering-datalake-dev/temp/" # Replace with your temporary directory
  bucket_name          = [module.s3.bucket_name]
  glue_job_script_path = var.glue_job_script_path
  glue_script_s3       = var.glue_script_s3
  glue_python_function_path = var.glue_python_function_path
  # glue_job_default_arguments = var.glue_job_default_arguments
}
 
module "step-function" {
  source              = "./terraform-modules/step-function"
  step_function_name  = var.step_function_name
  lambda_function_arn = module.lambda_function.lambda_function_arn
  sqs_queue_arn       = module.sqs.sqs_queue_arn
  sns_topic_arn       = module.sns_topic.sns_topic_arn
  glue_job_name       = var.glue_job_name
  glue_crawler_name   = var.glue_crawler_name
  glue_crawler_arn    = module.glue.glue_crawler_arn[0]
  bucket_name = [module.s3.bucket_name,module.s3.processed_bucket,module.s3.analytics_bucket]
  CI-DataEngineeringPolicy_arn = var.CI-DataEngineeringPolicy_arn
}
 
module "dynamodb"{
  source = "./terraform-modules/dynamodb"
  file_processing_audit_table_name = var.file_processing_audit_table_name
  glue_job_audit_table_name = var.glue_job_audit_table_name
}
 
==================
tfvars

region                     = "us-west-2"
shared_credentials_files   = "/home/ubuntu/.aws/credentials"
shared_config_files        = "/home/ubuntu/.aws/config"
profile                    = "Non-Prod-DataEngineeringRole"
name                       = "dataengineering"
environment                = "dev"
bucket_name                = ["pse-datalake-source-420393866958-us-west-2-tf","pse-sapecc-dl-processed-dev-tf",
"pse-sapecc-dl-analytics-dev-tf","pse-sapecc-dl-dqlogs-dev-tf",
"pse-esri-dl-dqlogs-dev-tf","pse-sapecc-glue-assets-tf"]
lambda_function_name       = ["pse-sapcee-validate-dev-tf","pse_data_quality_error_notification-tf"]
project_name               = "datalake"
work_order                 = "143006900"
log_retention_days = 30
sns_topic_arn      = "arn:aws:sns:us-west-2:420393866958:pse-error-notification-tf"
default_s3_content = ["ausp_function_location", "ausp", "cabn", "circuit", "equipment"]
sqs_name = "pse-s3-event-sqs-tf"
max_recievecount = 3
timeout = 300
sqs_queue_arn = "arn:aws:sqs:us-west-2:420393866958:pse-s3-event-sqs-tf"
sqs_queue_url = "https://sqs.us-west-2.amazonaws.com/420393866958/pse-s3-event-sqs-tf"
#event_rule_name =  "pse-s3-event-sqs-dev_tf"
event_rule_name =  "pse-event-dl-upload-trigger-tf"
sns_topic_name = "pse-error-notification-tf"
 
 
# Lambda variables
lambda_handler       = "pse-sapcee-validate-dev.lambda_handler"
lambda_runtime       = "python3.8"
#lambda environment variables
filelocparam  = "/pse/sapecc/filepaths"
step_function_arn = "arn:aws:states:us-west-2:420393866958:stateMachine:pse-sap-dl-statemachine-dev-tf"
CI-DataEngineeringPolicy_arn  = "arn:aws:iam::420393866958:policy/CI-DataEngineeringPolicy"
lambda_function_role_name = ["pse-sapcee-validate-dev-role-tf","pse_data_quality_error_notification-role-tf"]
lambda_function_policy_name   = ["pse-sapcee-validate-dev-policy-tf","pse_data_quality_error_notification-policy-tf"]
subscription_email = "soma.pattnaik@pse.com"  # once deployed and tested will add client mail id
step_function_name = ["pse-sap-dl-statemachine-dev-tf","pse-esri-dl-statemachine-dev-tf"]
dynamodb_table_name = ["pse_dl_file_processing_audit-tf", "pse_dl_glue_job_audits-tf"]
 
#Glue Variables
glue_crawler_name = ["pse-sapecc-s3-raw-crawler-tf"]
glue_database_name = ["pse-sapecc-raw-db-tf","pse-sapecc-processed-db-tf","pse-sapecc-analytics-db-tf"]
glue_job_name = ["sap_processed_circuit_data-tf","sap_analytics_circuit_data-tf",
"sap_processed_consolidated_data-tf","sap_analytics_consolidated_data-tf"]
glue_job_script_path = ["sap_analytics_circuit_data.py",
"sap_analytics_consolidated_data.py",
"sap_processed_circuit_data.py",
"sap_processed_consolidated_data.py", "common_functions.py"]
 
#DynamoDB Variables
file_processing_audit_table_name = "pse_dl_file_processing_audit-tf"
glue_job_audit_table_name = "pse_dl_glue_job_audits-tf"
 
glue_script_s3 = "s3://pse-sapecc-glue-assets-tf/scripts"
glue_python_function_path = "s3://pse-sapecc-glue-assets-tf/utility"
glue_script_location_source_key = ["scripts/sap_analytics_circuit_data.py",
"scripts/sap_analytics_consolidated_data.py",
"scripts/sap_processed_circuit_data.py",
"scripts/sap_processed_consolidated_data.py", "utility/common_functions.py"]
 
glue_script_location_source_path = ["sap_analytics_circuit_data.py",
"sap_analytics_consolidated_data.py",
"sap_processed_circuit_data.py",
"sap_processed_consolidated_data.py", "common_functions.py"]
 
 
 ==========================
variables.tf
variable "region" {
  description = "the AWS region"
  type        = string
  # default = "us-west-2"
}
 
variable "shared_config_files" {
  description = "shared_config_files"
  type        = string
  # default = "/home/ubuntu/.aws/config"
}
 
variable "shared_credentials_files" {
  description = "shared_credentials_files"
  type        = string
  #  default = "/home/ubuntu/.aws/credentials"
}
 
variable "profile" {
  description = "profile"
  type        = string
  default = "Non-Prod-DataEngineeringRole"
}
 
variable "name" {
  description = "the name of your stack, e.g. \"datalake\""
  type        = string
  default = "dataengineering"
}
 
variable "environment" {
  description = "the name of your environment, e.g. \"dev\""
  type        = string
  default     = "dev"
}
 
variable "bucket_name" {
  description = "the name of your bucket name, e.g. \"datalake\""
  type        = list(string)
}
 
variable "project_name" {
  description = "the name of your bucket name, e.g. \"datalake\""
  type        = string
  default = "datalake"
}
 
variable "work_order" {
  description = "the name of your bucket name, e.g. \"datalake\""
  type        = string
}
 
variable "default_s3_content" {
  description = "The default content of the s3 bucket upon creation of the bucket"
  type        = set(string)
}
 
variable "sns_topic_arn" {
  description = "SNS topic ARN for CloudWatch alarm notifications"
  type        = string
}
 
variable "log_retention_days" {
  description = "Number of days to retain logs in CloudWatch"
  type        = number
  default     = 14
}
variable "lambda_function_name" {
  description = "The name of the lambda function"
  type        = list(string)
}
 
variable "lambda_function_role_name" {
  description = "The name of the lambda function"
  type        = list(string)
}
 
 
variable "lambda_function_policy_name" {
  description = "The name of the lambda function"
  type        = list(string)
}
 
variable "lambda_runtime" {
  description = "The runtime for the lambda function"
  type        = string
}
 
 
variable "lambda_handler" {
  description = "The handler for the lambda function"
  type        = string
}
variable "glue_database_name" {
  description = "The S3 path to the Python script that AWS Glue executes."
  type        = list(string)
}
variable "step_function_name" {
  description = "The S3 path to the Python script that AWS Glue executes."
  type        = list(string)
}
 
variable "glue_job_name" {
  description = "the name of your bucket name, e.g. \"datalake\""
  type        = list(string)
}
 
variable "glue_crawler_name" {
  description = "The S3 path to the Python script that AWS Glue executes."
  type        = list(string)
}
 
variable "dynamodb_table_name" {
  type = list(any)
}
 
variable "glue_job_script_path" {
  description = "The S3 path to the Python script that AWS Glue executes."
  type        = list(string)
}
 
variable "file_processing_audit_table_name" {
  description = "Name of the DynamoDB table for file processing audit."
  type        = string
  # default     = "pse_dl_file_processing_audit"
}
 
variable "glue_job_audit_table_name" {
  description = "Name of the DynamoDB table for Glue job audit."
  type        = string
  # default     = "pse_dl_glue_job_audits"
}
variable "glue_script_s3" {
  description = "The S3 bucket path to the Python script that AWS Glue executes."
  type        = string
}
 
variable "glue_python_function_path" {
   description = "The S3 bucket to the Python script that AWS Glue executes."
  type        = string
}
# variable "glue_python_library_path" {
#    description = "The S3 bucket to the Python script that glue job use for library."
#    type        = string
# }
 
variable "CI-DataEngineeringPolicy_arn" {
  description = "arn of the role policy"
  type = string
}
 
//  variable "glue_job_default_arguments" {
//   description = "Default arguments for the Glue jobs, including specific settings for each job."
//     type        =   list(map(string))
// }
# variable "delay_seconds" {
#   description = "The time in seconds for which the delivery of all messages in the queue is delayed."
#   type        = number
#   default     = 0
# }
 
// variable "log_retention_days" {
//   description = "Number of days to retain logs in CloudWatch"
//   type        = number
//   default     = 14
// }
// # variable "max_message_size" {
// #   description = "The limit of how many bytes a message can contain before Amazon SQS rejects it."
// #   type        = number
// #   default     = 262144
// # }
 
// # variable "message_retention_seconds" {
// #   description = "The number of seconds Amazon SQS retains a message."
// #   type        = number
// #   default     = 345600
// # }
 
// # variable "receive_wait_time_seconds" {
// #   description = "The time for which a ReceiveMessage call waits for a message to arrive."
// #   type        = number
// #   default     = 0
// # }
 
// # variable "visibility_timeout_seconds" {
// #   description = "The visibility timeout for the queue."
// #   type        = number
// #   default     = 30
// # }
 
// # variable "tags" {
// #   description = "A map of tags to assign to the resource."
// #   type        = map(string)
// #   default = {
// #     Environment = "production"
// #     Application = "my-app"
// #   }
// # }
 
variable "event_rule_name" {
  description = "The name of the EventBridge rule."
  type        = string
}
 
// # variable "event_target_id" {
// #   description = "The ID of the EventBridge target."
// #   type        = string
// # }
 
variable "sqs_queue_arn" {
  type = string
}
 
variable "sqs_queue_url" {
  type = string
}
 
variable "subscription_email" {
  description = "Email address for the SNS subscription"
  type        = string
}
 
variable "sns_topic_name" {
  description = "Name for the SNS Topic"
  type        = string
}
 
# variable "lambda_role_arn" {
#   description = "The ARN of the IAM role that Lambda assumes when it executes the function"
#   type        = list(string)
# }
 
// variable "lambda_function_cloud_watch" {
//   description = "The name of the IAM policy for the lambda function"
//   type        = string
// }
 
// variable "sns_topic_arn" {
//   description = "SNS topic ARN for CloudWatch alarm notifications"
//   type        = string
// }
# variable "lambda_policy_json" {
#   description = "The JSON policy document for the lambda function"
#   type        = string
# }
 
# variable "lambda_handler" {
#   description = "The handler for the lambda function"
#   type        = string
# }
 
# variable "lambda_runtime" {
#   description = "The runtime for the lambda function"
#   type        = string
# }
 
 
 
# variable "s3_bucket_arn" {
#   description = "The ARN of the S3 bucket to trigger the lambda function"
#   type        = string
# }
 
# variable "s3_filter_prefix" {
#   description = "The prefix filter for the S3 bucket notifications"
#   type        = string
#   default     = ""
# }
 
# variable "s3_filter_suffix" {
#   description = "The suffix filter for the S3 bucket notifications"
#   type        = string
#   default     = ""
# }
 
variable "glue_script_location_source_key" {
  description = "the name of your bucket name, e.g. \"datalake\""
  type = list(string)
}
 
variable "glue_script_location_source_path" {
  description = "the name of your bucket name, e.g. \"datalake\""
  type = list(string)
}
variable "sqs_name" {
  type = string
}
variable "max_recievecount" {
  type    = number
  default = 3
}
variable "timeout" {
  type = number
}
 
#  variable "bucket_name" {
#    description = "The name of the S3 bucket to trigger the lambda function"
#    type        = list(string)
#  }
 
 
 
 
variable "filelocparam" {
  description = "The name of the S3 bucket to trigger the lambda function"
  type        = string 
  
}
 
// variable "sns_topic_arn" {
//   description = "The name of the S3 bucket to trigger the lambda function"
//   type        = string 
  
// }
variable "step_function_arn" {
  description = "The name of the S3 bucket to trigger the lambda function"
  type        = string 
  
}
 
